4张显卡跑代码
node04               Mon Aug 28 19:16:30 2023  515.65.01
[0] NVIDIA A40       | 35'C,   0 % |   573 / 46068 MB |
[1] NVIDIA A40       | 34'C,   0 % |   573 / 46068 MB |
[2] NVIDIA A40       | 34'C,   0 % |   573 / 46068 MB |
[3] NVIDIA A40       | 34'C,   0 % |   573 / 46068 MB |
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
  warning_cache.warn(
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type                          | Params
--------------------------------------------------------
0 | model | BertForSequenceClassification | 109 M 
--------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.947   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
开始加载模型
MyBERTModel(
  (model): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=6, bias=True)
  )
)
模型获取成功
**************************************************
数据开始加载
已经map了啊
数据加载成功
**************************************************
模型开始训练
{'label': tensor(0), 'input_ids': tensor([  101,  1045,  2293,  2336,  1055,  3906,  6048,  2040,  2123,  1056,
         2514,  1996,  2342,  2000, 12873,  2091,  2477,  2005,  4268,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]                                                                           Training: 0it [00:00, ?it/s]Training:   0%|          | 0/15 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s] Epoch 0:   7%|▋         | 1/15 [00:00<00:12,  1.11it/s]Epoch 0:   7%|▋         | 1/15 [00:00<00:12,  1.09it/s, v_num=2, train_loss_step=1.810, test_acc_step=0.148]Epoch 0:  13%|█▎        | 2/15 [00:01<00:11,  1.16it/s, v_num=2, train_loss_step=1.810, test_acc_step=0.148]Epoch 0:  13%|█▎        | 2/15 [00:01<00:11,  1.15it/s, v_num=2, train_loss_step=1.660, test_acc_step=0.352]Epoch 0:  20%|██        | 3/15 [00:02<00:10,  1.18it/s, v_num=2, train_loss_step=1.660, test_acc_step=0.352]Epoch 0:  20%|██        | 3/15 [00:02<00:10,  1.17it/s, v_num=2, train_loss_step=2.230, test_acc_step=0.289]Epoch 0:  27%|██▋       | 4/15 [00:03<00:09,  1.18it/s, v_num=2, train_loss_step=2.230, test_acc_step=0.289]Epoch 0:  27%|██▋       | 4/15 [00:03<00:09,  1.18it/s, v_num=2, train_loss_step=2.100, test_acc_step=0.250]Epoch 0:  33%|███▎      | 5/15 [00:04<00:08,  1.19it/s, v_num=2, train_loss_step=2.100, test_acc_step=0.250]Epoch 0:  33%|███▎      | 5/15 [00:04<00:08,  1.18it/s, v_num=2, train_loss_step=1.760, test_acc_step=0.328]Epoch 0:  40%|████      | 6/15 [00:05<00:07,  1.19it/s, v_num=2, train_loss_step=1.760, test_acc_step=0.328]Epoch 0:  40%|████      | 6/15 [00:05<00:07,  1.18it/s, v_num=2, train_loss_step=1.660, test_acc_step=0.312]Epoch 0:  47%|████▋     | 7/15 [00:05<00:06,  1.19it/s, v_num=2, train_loss_step=1.660, test_acc_step=0.312]Epoch 0:  47%|████▋     | 7/15 [00:05<00:06,  1.19it/s, v_num=2, train_loss_step=1.720, test_acc_step=0.324]Epoch 0:  53%|█████▎    | 8/15 [00:06<00:05,  1.19it/s, v_num=2, train_loss_step=1.720, test_acc_step=0.324]Epoch 0:  53%|█████▎    | 8/15 [00:06<00:05,  1.19it/s, v_num=2, train_loss_step=1.670, test_acc_step=0.344]Epoch 0:  60%|██████    | 9/15 [00:07<00:05,  1.19it/s, v_num=2, train_loss_step=1.670, test_acc_step=0.344]Epoch 0:  60%|██████    | 9/15 [00:07<00:05,  1.19it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.336]Epoch 0:  67%|██████▋   | 10/15 [00:08<00:04,  1.20it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.336]Epoch 0:  67%|██████▋   | 10/15 [00:08<00:04,  1.19it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.375]Epoch 0:  73%|███████▎  | 11/15 [00:09<00:03,  1.20it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.375]Epoch 0:  73%|███████▎  | 11/15 [00:09<00:03,  1.19it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.309]Epoch 0:  80%|████████  | 12/15 [00:10<00:02,  1.20it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.309]Epoch 0:  80%|████████  | 12/15 [00:10<00:02,  1.20it/s, v_num=2, train_loss_step=1.520, test_acc_step=0.316]Epoch 0:  87%|████████▋ | 13/15 [00:10<00:01,  1.20it/s, v_num=2, train_loss_step=1.520, test_acc_step=0.316]Epoch 0:  87%|████████▋ | 13/15 [00:10<00:01,  1.20it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.332]Epoch 0:  93%|█████████▎| 14/15 [00:11<00:00,  1.20it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.332]Epoch 0:  93%|█████████▎| 14/15 [00:11<00:00,  1.20it/s, v_num=2, train_loss_step=1.620, test_acc_step=0.297]Epoch 0: 100%|██████████| 15/15 [00:12<00:00,  1.20it/s, v_num=2, train_loss_step=1.620, test_acc_step=0.297]Epoch 0: 100%|██████████| 15/15 [00:12<00:00,  1.20it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.254]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.49it/s][AEpoch 0: 100%|██████████| 15/15 [00:12<00:00,  1.17it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.359, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.359]
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
                                                                      [AEpoch 0: 100%|██████████| 15/15 [00:12<00:00,  1.17it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.359, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 0:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.359, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]         Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.359, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:   7%|▋         | 1/15 [00:00<00:12,  1.09it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.359, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:   7%|▋         | 1/15 [00:00<00:13,  1.07it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.336, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  13%|█▎        | 2/15 [00:01<00:11,  1.15it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.336, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  13%|█▎        | 2/15 [00:01<00:11,  1.14it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.328, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  20%|██        | 3/15 [00:02<00:10,  1.17it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.328, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  20%|██        | 3/15 [00:02<00:10,  1.16it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.320, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  27%|██▋       | 4/15 [00:03<00:09,  1.18it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.320, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  27%|██▋       | 4/15 [00:03<00:09,  1.17it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.359, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  33%|███▎      | 5/15 [00:04<00:08,  1.18it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.359, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  33%|███▎      | 5/15 [00:04<00:08,  1.18it/s, v_num=2, train_loss_step=1.550, test_acc_step=0.270, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  40%|████      | 6/15 [00:05<00:07,  1.19it/s, v_num=2, train_loss_step=1.550, test_acc_step=0.270, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  40%|████      | 6/15 [00:05<00:07,  1.18it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.305, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  47%|████▋     | 7/15 [00:05<00:06,  1.19it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.305, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  47%|████▋     | 7/15 [00:05<00:06,  1.19it/s, v_num=2, train_loss_step=1.680, test_acc_step=0.277, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  53%|█████▎    | 8/15 [00:06<00:05,  1.19it/s, v_num=2, train_loss_step=1.680, test_acc_step=0.277, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  53%|█████▎    | 8/15 [00:06<00:05,  1.19it/s, v_num=2, train_loss_step=1.620, test_acc_step=0.262, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  60%|██████    | 9/15 [00:07<00:05,  1.19it/s, v_num=2, train_loss_step=1.620, test_acc_step=0.262, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  60%|██████    | 9/15 [00:07<00:05,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.355, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  67%|██████▋   | 10/15 [00:08<00:04,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.355, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  67%|██████▋   | 10/15 [00:08<00:04,  1.19it/s, v_num=2, train_loss_step=1.620, test_acc_step=0.340, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  73%|███████▎  | 11/15 [00:09<00:03,  1.19it/s, v_num=2, train_loss_step=1.620, test_acc_step=0.340, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  73%|███████▎  | 11/15 [00:09<00:03,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.340, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  80%|████████  | 12/15 [00:10<00:02,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.340, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  80%|████████  | 12/15 [00:10<00:02,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.301, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  87%|████████▋ | 13/15 [00:10<00:01,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.301, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  87%|████████▋ | 13/15 [00:10<00:01,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.309, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  93%|█████████▎| 14/15 [00:11<00:00,  1.20it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.309, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1:  93%|█████████▎| 14/15 [00:11<00:00,  1.19it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.316, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1: 100%|██████████| 15/15 [00:12<00:00,  1.20it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.316, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]Epoch 1: 100%|██████████| 15/15 [00:12<00:00,  1.20it/s, v_num=2, train_loss_step=2.120, test_acc_step=0.129, val_loss_step=1.630, val_loss_epoch=1.630, test_acc_epoch=0.304, train_loss_epoch=1.720]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s][AEpoch 1: 100%|██████████| 15/15 [00:12<00:00,  1.17it/s, v_num=2, train_loss_step=2.120, test_acc_step=0.336, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.336, train_loss_epoch=1.720]
                                                                      [AEpoch 1: 100%|██████████| 15/15 [00:12<00:00,  1.17it/s, v_num=2, train_loss_step=2.120, test_acc_step=0.336, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=2.120, test_acc_step=0.336, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]         Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=2.120, test_acc_step=0.336, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:   7%|▋         | 1/15 [00:00<00:12,  1.08it/s, v_num=2, train_loss_step=2.120, test_acc_step=0.336, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:   7%|▋         | 1/15 [00:00<00:13,  1.07it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.332, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  13%|█▎        | 2/15 [00:01<00:11,  1.14it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.332, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  13%|█▎        | 2/15 [00:01<00:11,  1.13it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.340, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  20%|██        | 3/15 [00:02<00:10,  1.16it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.340, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  20%|██        | 3/15 [00:02<00:10,  1.16it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.316, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  27%|██▋       | 4/15 [00:03<00:09,  1.17it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.316, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  27%|██▋       | 4/15 [00:03<00:09,  1.17it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.355, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  33%|███▎      | 5/15 [00:04<00:08,  1.18it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.355, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  33%|███▎      | 5/15 [00:04<00:08,  1.18it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.289, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  40%|████      | 6/15 [00:05<00:07,  1.18it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.289, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  40%|████      | 6/15 [00:05<00:07,  1.18it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.270, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  47%|████▋     | 7/15 [00:05<00:06,  1.18it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.270, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  47%|████▋     | 7/15 [00:05<00:06,  1.18it/s, v_num=2, train_loss_step=1.690, test_acc_step=0.312, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  53%|█████▎    | 8/15 [00:06<00:05,  1.19it/s, v_num=2, train_loss_step=1.690, test_acc_step=0.312, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  53%|█████▎    | 8/15 [00:06<00:05,  1.18it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.328, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  60%|██████    | 9/15 [00:07<00:05,  1.19it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.328, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  60%|██████    | 9/15 [00:07<00:05,  1.19it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.352, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  67%|██████▋   | 10/15 [00:08<00:04,  1.19it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.352, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  67%|██████▋   | 10/15 [00:08<00:04,  1.19it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.289, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  73%|███████▎  | 11/15 [00:09<00:03,  1.19it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.289, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  73%|███████▎  | 11/15 [00:09<00:03,  1.19it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.332, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  80%|████████  | 12/15 [00:10<00:02,  1.19it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.332, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  80%|████████  | 12/15 [00:10<00:02,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.312, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  87%|████████▋ | 13/15 [00:10<00:01,  1.19it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.312, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  87%|████████▋ | 13/15 [00:10<00:01,  1.19it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.355, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  93%|█████████▎| 14/15 [00:11<00:00,  1.19it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.355, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2:  93%|█████████▎| 14/15 [00:11<00:00,  1.19it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.332, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2: 100%|██████████| 15/15 [00:12<00:00,  1.19it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.332, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]Epoch 2: 100%|██████████| 15/15 [00:12<00:00,  1.19it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.301, val_loss_step=1.610, val_loss_epoch=1.610, test_acc_epoch=0.303, train_loss_epoch=1.650]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.47it/s][AEpoch 2: 100%|██████████| 15/15 [00:12<00:00,  1.16it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.340, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.340, train_loss_epoch=1.650]
                                                                      [AEpoch 2: 100%|██████████| 15/15 [00:12<00:00,  1.16it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.340, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 2:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.340, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]         Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.340, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:   7%|▋         | 1/15 [00:00<00:12,  1.09it/s, v_num=2, train_loss_step=1.640, test_acc_step=0.340, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:   7%|▋         | 1/15 [00:00<00:13,  1.07it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.281, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  13%|█▎        | 2/15 [00:01<00:11,  1.14it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.281, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  13%|█▎        | 2/15 [00:01<00:11,  1.13it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.328, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  20%|██        | 3/15 [00:02<00:10,  1.16it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.328, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  20%|██        | 3/15 [00:02<00:10,  1.15it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.301, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  27%|██▋       | 4/15 [00:03<00:09,  1.17it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.301, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  27%|██▋       | 4/15 [00:03<00:09,  1.16it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.273, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  33%|███▎      | 5/15 [00:04<00:08,  1.17it/s, v_num=2, train_loss_step=1.610, test_acc_step=0.273, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  33%|███▎      | 5/15 [00:04<00:08,  1.17it/s, v_num=2, train_loss_step=1.540, test_acc_step=0.344, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  40%|████      | 6/15 [00:05<00:07,  1.18it/s, v_num=2, train_loss_step=1.540, test_acc_step=0.344, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  40%|████      | 6/15 [00:05<00:07,  1.17it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.352, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  47%|████▋     | 7/15 [00:05<00:06,  1.18it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.352, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  47%|████▋     | 7/15 [00:05<00:06,  1.18it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.344, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  53%|█████▎    | 8/15 [00:06<00:05,  1.18it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.344, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  53%|█████▎    | 8/15 [00:06<00:05,  1.18it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.344, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  60%|██████    | 9/15 [00:07<00:05,  1.18it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.344, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  60%|██████    | 9/15 [00:07<00:05,  1.18it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.355, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  67%|██████▋   | 10/15 [00:08<00:04,  1.18it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.355, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  67%|██████▋   | 10/15 [00:08<00:04,  1.18it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.316, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  73%|███████▎  | 11/15 [00:09<00:03,  1.18it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.316, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  73%|███████▎  | 11/15 [00:09<00:03,  1.18it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.242, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  80%|████████  | 12/15 [00:10<00:02,  1.18it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.242, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  80%|████████  | 12/15 [00:10<00:02,  1.18it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.301, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  87%|████████▋ | 13/15 [00:10<00:01,  1.18it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.301, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  87%|████████▋ | 13/15 [00:10<00:01,  1.18it/s, v_num=2, train_loss_step=1.530, test_acc_step=0.363, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  93%|█████████▎| 14/15 [00:11<00:00,  1.19it/s, v_num=2, train_loss_step=1.530, test_acc_step=0.363, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3:  93%|█████████▎| 14/15 [00:11<00:00,  1.18it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.359, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3: 100%|██████████| 15/15 [00:12<00:00,  1.19it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.359, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]Epoch 3: 100%|██████████| 15/15 [00:12<00:00,  1.19it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.336, val_loss_step=1.550, val_loss_epoch=1.550, test_acc_epoch=0.321, train_loss_epoch=1.610]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s][AEpoch 3: 100%|██████████| 15/15 [00:12<00:00,  1.16it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.352, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.352, train_loss_epoch=1.610]
                                                                      [AEpoch 3: 100%|██████████| 15/15 [00:12<00:00,  1.16it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.352, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 3:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.352, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]         Epoch 4:   0%|          | 0/15 [00:00<?, ?it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.352, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:   7%|▋         | 1/15 [00:00<00:13,  1.06it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.352, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:   7%|▋         | 1/15 [00:00<00:13,  1.05it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.324, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  13%|█▎        | 2/15 [00:01<00:11,  1.12it/s, v_num=2, train_loss_step=1.630, test_acc_step=0.324, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  13%|█▎        | 2/15 [00:01<00:11,  1.11it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.340, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  20%|██        | 3/15 [00:02<00:10,  1.14it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.340, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  20%|██        | 3/15 [00:02<00:10,  1.14it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.328, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  27%|██▋       | 4/15 [00:03<00:09,  1.16it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.328, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  27%|██▋       | 4/15 [00:03<00:09,  1.15it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.348, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  33%|███▎      | 5/15 [00:04<00:08,  1.16it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.348, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  33%|███▎      | 5/15 [00:04<00:08,  1.16it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.309, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  40%|████      | 6/15 [00:05<00:07,  1.16it/s, v_num=2, train_loss_step=1.650, test_acc_step=0.309, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  40%|████      | 6/15 [00:05<00:07,  1.16it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.371, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  47%|████▋     | 7/15 [00:05<00:06,  1.17it/s, v_num=2, train_loss_step=1.560, test_acc_step=0.371, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  47%|████▋     | 7/15 [00:06<00:06,  1.17it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.297, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  53%|█████▎    | 8/15 [00:06<00:05,  1.17it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.297, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  53%|█████▎    | 8/15 [00:06<00:05,  1.17it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.297, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  60%|██████    | 9/15 [00:07<00:05,  1.17it/s, v_num=2, train_loss_step=1.600, test_acc_step=0.297, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  60%|██████    | 9/15 [00:07<00:05,  1.17it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.379, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  67%|██████▋   | 10/15 [00:08<00:04,  1.18it/s, v_num=2, train_loss_step=1.580, test_acc_step=0.379, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  67%|██████▋   | 10/15 [00:08<00:04,  1.17it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.355, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  73%|███████▎  | 11/15 [00:09<00:03,  1.18it/s, v_num=2, train_loss_step=1.590, test_acc_step=0.355, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  73%|███████▎  | 11/15 [00:09<00:03,  1.18it/s, v_num=2, train_loss_step=1.510, test_acc_step=0.375, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  80%|████████  | 12/15 [00:10<00:02,  1.18it/s, v_num=2, train_loss_step=1.510, test_acc_step=0.375, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  80%|████████  | 12/15 [00:10<00:02,  1.18it/s, v_num=2, train_loss_step=1.540, test_acc_step=0.375, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  87%|████████▋ | 13/15 [00:11<00:01,  1.18it/s, v_num=2, train_loss_step=1.540, test_acc_step=0.375, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  87%|████████▋ | 13/15 [00:11<00:01,  1.18it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.352, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  93%|█████████▎| 14/15 [00:11<00:00,  1.18it/s, v_num=2, train_loss_step=1.570, test_acc_step=0.352, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4:  93%|█████████▎| 14/15 [00:11<00:00,  1.18it/s, v_num=2, train_loss_step=1.660, test_acc_step=0.305, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4: 100%|██████████| 15/15 [00:12<00:00,  1.18it/s, v_num=2, train_loss_step=1.660, test_acc_step=0.305, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]Epoch 4: 100%|██████████| 15/15 [00:12<00:00,  1.18it/s, v_num=2, train_loss_step=1.550, test_acc_step=0.328, val_loss_step=1.570, val_loss_epoch=1.570, test_acc_epoch=0.323, train_loss_epoch=1.580]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s][AEpoch 4: 100%|██████████| 15/15 [00:13<00:00,  1.15it/s, v_num=2, train_loss_step=1.550, test_acc_step=0.305, val_loss_step=1.600, val_loss_epoch=1.600, test_acc_epoch=0.305, train_loss_epoch=1.580]
`Trainer.fit` stopped: `max_epochs=5` reached.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
                                                                      [AEpoch 4: 100%|██████████| 15/15 [00:13<00:00,  1.15it/s, v_num=2, train_loss_step=1.550, test_acc_step=0.305, val_loss_step=1.600, val_loss_epoch=1.600, test_acc_epoch=0.339, train_loss_epoch=1.580]Epoch 4: 100%|██████████| 15/15 [00:15<00:00,  1.02s/it, v_num=2, train_loss_step=1.550, test_acc_step=0.305, val_loss_step=1.600, val_loss_epoch=1.600, test_acc_epoch=0.339, train_loss_epoch=1.580]
模型训练成功
**************************************************
开始进行test
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:486: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
Testing: 0it [00:00, ?it/s]Testing:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │         0.234375          │
│      test_loss_epoch      │    1.6090435981750488     │
└───────────────────────────┴───────────────────────────┘
这个工作完成啦
