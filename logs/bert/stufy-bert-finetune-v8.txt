4张显卡跑代码
node09               Tue Aug 29 16:01:22 2023  515.65.01
[0] NVIDIA A40       | 47'C,   0 % |   573 / 46068 MB |
[1] NVIDIA A40       | 49'C,   0 % |   573 / 46068 MB |
[2] NVIDIA A40       | 46'C,   0 % |   573 / 46068 MB |
[3] NVIDIA A40       | 46'C,   0 % |   573 / 46068 MB |
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using the latest cached version of the module from /home/jiangyunqi/.cache/huggingface/modules/datasets_modules/datasets/dair-ai--emotion/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd (last modified on Fri Aug 25 13:45:14 2023) since it couldn't be found locally at dair-ai/emotion., or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/jiangyunqi/.cache/huggingface/modules/datasets_modules/datasets/dair-ai--emotion/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd (last modified on Fri Aug 25 13:45:14 2023) since it couldn't be found locally at dair-ai/emotion., or remotely on the Hugging Face Hub.
Using the latest cached version of the module from /home/jiangyunqi/.cache/huggingface/modules/datasets_modules/datasets/dair-ai--emotion/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd (last modified on Fri Aug 25 13:45:14 2023) since it couldn't be found locally at dair-ai/emotion., or remotely on the Hugging Face Hub.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
  warning_cache.warn(
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type                          | Params
--------------------------------------------------------
0 | model | BertForSequenceClassification | 109 M 
--------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.947   Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
开始加载模型
MyBERTModel(
  (model): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=6, bias=True)
  )
)
模型获取成功
**************************************************
数据开始加载
已经map了啊
数据加载成功
**************************************************
模型开始训练
{'label': tensor(0), 'input_ids': tensor([  101,  1045,  2293,  2336,  1055,  3906,  6048,  2040,  2123,  1056,
         2514,  1996,  2342,  2000, 12873,  2091,  2477,  2005,  4268,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
开始加载模型
MyBERTModel(
  (model): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=6, bias=True)
  )
)
模型获取成功
**************************************************
数据开始加载
已经map了啊
数据加载成功
**************************************************
模型开始训练
{'label': tensor(0), 'input_ids': tensor([  101,  1045,  2293,  2336,  1055,  3906,  6048,  2040,  2123,  1056,
         2514,  1996,  2342,  2000, 12873,  2091,  2477,  2005,  4268,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
开始加载模型
MyBERTModel(
  (model): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=6, bias=True)
  )
)
模型获取成功
**************************************************
数据开始加载
已经map了啊
数据加载成功
**************************************************
模型开始训练
{'label': tensor(0), 'input_ids': tensor([  101,  1045,  2293,  2336,  1055,  3906,  6048,  2040,  2123,  1056,
         2514,  1996,  2342,  2000, 12873,  2091,  2477,  2005,  4268,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
开始加载模型
MyBERTModel(
  (model): BertForSequenceClassification(
    (bert): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0-11): 12 x BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
              (intermediate_act_fn): GELUActivation()
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
    (dropout): Dropout(p=0.1, inplace=False)
    (classifier): Linear(in_features=768, out_features=6, bias=True)
  )
)
模型获取成功
**************************************************
数据开始加载
已经map了啊
数据加载成功
**************************************************
模型开始训练
{'label': tensor(0), 'input_ids': tensor([  101,  1045,  2293,  2336,  1055,  3906,  6048,  2040,  2123,  1056,
         2514,  1996,  2342,  2000, 12873,  2091,  2477,  2005,  4268,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/utilities/data.py:103: UserWarning: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Sanity Checking: 0it [00:00, ?it/s]                                   Training: 0it [00:00, ?it/s]Training:   0%|          | 0/7 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s] Epoch 0:  14%|█▍        | 1/7 [00:02<00:17,  2.98s/it]Epoch 0:  14%|█▍        | 1/7 [00:03<00:19,  3.20s/it, v_num=52, train_loss_step=1.740, train_acc_step=0.330]Epoch 0:  29%|██▊       | 2/7 [00:04<00:11,  2.40s/it, v_num=52, train_loss_step=1.740, train_acc_step=0.330]Epoch 0:  29%|██▊       | 2/7 [00:04<00:12,  2.43s/it, v_num=52, train_loss_step=1.680, train_acc_step=0.293]Epoch 0:  43%|████▎     | 3/7 [00:06<00:08,  2.14s/it, v_num=52, train_loss_step=1.680, train_acc_step=0.293]Epoch 0:  43%|████▎     | 3/7 [00:06<00:08,  2.17s/it, v_num=52, train_loss_step=1.800, train_acc_step=0.297]Epoch 0:  57%|█████▋    | 4/7 [00:08<00:06,  2.01s/it, v_num=52, train_loss_step=1.800, train_acc_step=0.297]Epoch 0:  57%|█████▋    | 4/7 [00:08<00:06,  2/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('train_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
.03s/it, v_num=52, train_loss_step=1.630, train_acc_step=0.287]Epoch 0:  71%|███████▏  | 5/7 [00:09<00:03,  1.94s/it, v_num=52, train_loss_step=1.630, train_acc_step=0.287]Epoch 0:  71%|███████▏  | 5/7 [00:09<00:03,  1.95s/it, v_num=52, train_loss_step=1.710, train_acc_step=0.326]Epoch 0:  86%|████████▌ | 6/7 [00:11<00:01,  1.88s/it, v_num=52, train_loss_step=1.710, train_acc_step=0.326]Epoch 0:  86%|████████▌ | 6/7 [00:11<00:01,  1.90s/it, v_num=52, train_loss_step=2.780, train_acc_step=0.0918]Epoch 0: 100%|██████████| 7/7 [00:12<00:00,  1.85s/it, v_num=52, train_loss_step=2.780, train_acc_step=0.0918]Epoch 0: 100%|██████████| 7/7 [00:12<00:00,  1.86s/it, v_num=52, train_loss_step=1.660, train_acc_step=0.307] Epoch 0: 100%|██████████| 7/7 [00:12<00:00,  1.86s/it, v_num=52, train_loss_step=1.660, train_acc_step=0.307, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 0:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.660, train_acc_step=0.307, train_loss_epoch=1.860, train_acc_epoch=0.276]        Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.660, train_acc_step=0.307, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  14%|█▍        | 1/7 [00:01<00:10,  1.75s/it, v_num=52, train_loss_step=1.660, train_acc_step=0.307, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  14%|█▍        | 1/7 [00:01<00:10,  1.81s/it, v_num=52, train_loss_step=1.960, train_acc_step=0.293, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  29%|██▊       | 2/7 [00:03<00:08,  1.69s/it, v_num=52, train_loss_step=1.960, train_acc_step=0.293, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  29%|██▊       | 2/7 [00:03<00:08,  1.73s/it, v_num=52, train_loss_step=1.680, train_acc_step=0.295, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  43%|████▎     | 3/7 [00:05<00:06,  1.67s/it, v_num=52, train_loss_step=1.680, train_acc_step=0.295, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  43%|████▎     | 3/7 [00:05<00:06,  1.69s/it, v_num=52, train_loss_step=1.690, train_acc_step=0.311, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  57%|█████▋    | 4/7 [00:06<00:04,  1.66s/it, v_num=52, train_loss_step=1.690, train_acc_step=0.311, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  57%|█████▋    | 4/7 [00:06<00:05,  1.68s/it, v_num=52, train_loss_step=1.720, train_acc_step=0.295, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  71%|███████▏  | 5/7 [00:08<00:03,  1.66s/it, v_num=52, train_loss_step=1.720, train_acc_step=0.295, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  71%|███████▏  | 5/7 [00:08<00:03,  1.67s/it, v_num=52, train_loss_step=1.580, train_acc_step=0.326, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  86%|████████▌ | 6/7 [00:09<00:01,  1.65s/it, v_num=52, train_loss_step=1.580, train_acc_step=0.326, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1:  86%|████████▌ | 6/7 [00:09<00:01,  1.66s/it, v_num=52, train_loss_step=1.650, train_acc_step=0.273, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1: 100%|██████████| 7/7 [00:11<00:00,  1.65s/it, v_num=52, train_loss_step=1.650, train_acc_step=0.273, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1: 100%|██████████| 7/7 [00:11<00:00,  1.66s/it, v_num=52, train_loss_step=1.630, train_acc_step=0.295, train_loss_epoch=1.860, train_acc_epoch=0.276]Epoch 1: 100%|██████████| 7/7 [00:11<00:00,  1.66s/it, v_num=52, train_loss_step=1.630, train_acc_step=0.295, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 1:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.630, train_acc_step=0.295, train_loss_epoch=1.700, train_acc_epoch=0.298]        Epoch 2:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.630, train_acc_step=0.295, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  14%|█▍        | 1/7 [00:01<00:10,  1.73s/it, v_num=52, train_loss_step=1.630, train_acc_step=0.295, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  14%|█▍        | 1/7 [00:01<00:10,  1.80s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.309, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  29%|██▊       | 2/7 [00:03<00:08,  1.70s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.309, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  29%|██▊       | 2/7 [00:03<00:08,  1.73s/it, v_num=52, train_loss_step=1.550, train_acc_step=0.340, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  43%|████▎     | 3/7 [00:05<00:06,  1.67s/it, v_num=52, train_loss_step=1.550, train_acc_step=0.340, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  43%|████▎     | 3/7 [00:05<00:06,  1.70s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.350, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  57%|█████▋    | 4/7 [00:06<00:04,  1.66s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.350, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  57%|█████▋    | 4/7 [00:06<00:05,  1.68s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.330, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  71%|███████▏  | 5/7 [00:08<00:03,  1.66s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.330, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  71%|███████▏  | 5/7 [00:08<00:03,  1.67s/it, v_num=52, train_loss_step=1.570, train_acc_step=0.359, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  86%|████████▌ | 6/7 [00:09<00:01,  1.66s/it, v_num=52, train_loss_step=1.570, train_acc_step=0.359, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2:  86%|████████▌ | 6/7 [00:10<00:01,  1.67s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.332, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2: 100%|██████████| 7/7 [00:11<00:00,  1.65s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.332, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2: 100%|██████████| 7/7 [00:11<00:00,  1.66s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.295, train_loss_epoch=1.700, train_acc_epoch=0.298]Epoch 2: 100%|██████████| 7/7 [00:11<00:00,  1.66s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.295, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 2:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.590, train_acc_step=0.295, train_loss_epoch=1.590, train_acc_epoch=0.331]        Epoch 3:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.590, train_acc_step=0.295, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  14%|█▍        | 1/7 [00:01<00:10,  1.71s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.295, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  14%|█▍        | 1/7 [00:01<00:10,  1.78s/it, v_num=52, train_loss_step=1.620, train_acc_step=0.289, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  29%|██▊       | 2/7 [00:03<00:08,  1.68s/it, v_num=52, train_loss_step=1.620, train_acc_step=0.289, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  29%|██▊       | 2/7 [00:03<00:08,  1.72s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.299, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  43%|████▎     | 3/7 [00:05<00:06,  1.67s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.299, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  43%|████▎     | 3/7 [00:05<00:06,  1.70s/it, v_num=52, train_loss_step=1.580, train_acc_step=0.322, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  57%|█████▋    | 4/7 [00:06<00:05,  1.67s/it, v_num=52, train_loss_step=1.580, train_acc_step=0.322, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  57%|█████▋    | 4/7 [00:06<00:05,  1.69s/it, v_num=52, train_loss_step=1.550, train_acc_step=0.340, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  71%|███████▏  | 5/7 [00:08<00:03,  1.67s/it, v_num=52, train_loss_step=1.550, train_acc_step=0.340, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  71%|███████▏  | 5/7 [00:08<00:03,  1.69s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.316, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  86%|████████▌ | 6/7 [00:10<00:01,  1.67s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.316, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3:  86%|████████▌ | 6/7 [00:10<00:01,  1.68s/it, v_num=52, train_loss_step=1.580, train_acc_step=0.361, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3: 100%|██████████| 7/7 [00:11<00:00,  1.66s/it, v_num=52, train_loss_step=1.580, train_acc_step=0.361, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3: 100%|██████████| 7/7 [00:11<00:00,  1.67s/it, v_num=52, train_loss_step=1.540, train_acc_step=0.354, train_loss_epoch=1.590, train_acc_epoch=0.331]Epoch 3: 100%|██████████| 7/7 [00:11<00:00,  1.67s/it, v_num=52, train_loss_step=1.540, train_acc_step=0.354, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 3:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.540, train_acc_step=0.354, train_loss_epoch=1.580, train_acc_epoch=0.326]        Epoch 4:   0%|          | 0/7 [00:00<?, ?it/s, v_num=52, train_loss_step=1.540, train_acc_step=0.354, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  14%|█▍        | 1/7 [00:01<00:10,  1.73s/it, v_num=52, train_loss_step=1.540, train_acc_step=0.354, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  14%|█▍        | 1/7 [00:01<00:10,  1.79s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.328, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  29%|██▊       | 2/7 [00:03<00:08,  1.71s/it, v_num=52, train_loss_step=1.610, train_acc_step=0.328, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  29%|██▊       | 2/7 [00:03<00:08,  1.74s/it, v_num=52, train_loss_step=1.550, train_acc_step=0.348, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  43%|████▎     | 3/7 [00:05<00:06,  1.69s/it, v_num=52, train_loss_step=1.550, train_acc_step=0.348, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  43%|████▎     | 3/7 [00:05<00:06,  1.72s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.324, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  57%|█████▋    | 4/7 [00:06<00:05,  1.69s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.324, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  57%|█████▋    | 4/7 [00:06<00:05,  1.71s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.324, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  71%|███████▏  | 5/7 [00:08<00:03,  1.69s/it, v_num=52, train_loss_step=1.590, train_acc_step=0.324, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  71%|███████▏  | 5/7 [00:08<00:03,  1.71s/it, v_num=52, train_loss_step=1.570, train_acc_step=0.330, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  86%|████████▌ | 6/7 [00:10<00:01,  1.69s/it, v_num=52, train_loss_step=1.570, train_acc_step=0.330, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4:  86%|████████▌ | 6/7 [00:10<00:01,  1.70s/it, v_num=52, train_loss_step=1.510, train_acc_step=0.367, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4: 100%|██████████| 7/7 [00:11<00:00,  1.68s/it, v_num=52, train_loss_step=1.510, train_acc_step=0.367, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4: 100%|██████████| 7/7 [00:11<00:00,  1.69s/it, v_num=52, train_loss_step=1.620, train_acc_step=0.320, train_loss_epoch=1.580, train_acc_epoch=0.326]Epoch 4: 100%|█████████`Trainer.fit` stopped: `max_epochs=5` reached.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
█| 7/7 [00:11<00:00,  1.69s/it, v_num=52, train_loss_step=1.620, train_acc_step=0.320, train_loss_epoch=1.580, train_acc_epoch=0.335]Epoch 4: 100%|██████████| 7/7 [00:14<00:00,  2.01s/it, v_num=52, train_loss_step=1.620, train_acc_step=0.320, train_loss_epoch=1.580, train_acc_epoch=0.335]
模型训练成功
**************************************************
开始进行test
模型训练成功
**************************************************
开始进行test
模型训练成功
**************************************************
开始进行test
模型训练成功
**************************************************
开始进行test
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:486: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:226: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1, num_nodes=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 72 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/jiangyunqi/anaconda3/envs/study-llama-v2-finetune/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
Testing: 0it [00:00, ?it/s]Testing:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃        Test metric        ┃       DataLoader 0        ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│      test_acc_epoch       │        0.41796875         │
│      test_loss_epoch      │    1.5432249307632446     │
└───────────────────────────┴───────────────────────────┘
这个工作完成啦
